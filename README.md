# üöÄ Projeto Final PySpark: Relat√≥rio de Pedidos Cr√≠ticos

## üéì 1. Informa√ß√µes Essenciais

| Detalhe | Valor |
| :--- | :--- |
| **Disciplina** | Data Engineering Programming |
| **Professor** | Marcelo Barbosa Pinto |
| **Integrante(s)** | [Seu Nome Completo] |
| **RM** | [Seu N√∫mero de RM] |
| **Link do GitHub** | [COLE AQUI O LINK DO SEU REPOSIT√ìRIO P√öBLICO] |

---

## 2. üéØ Escopo de Neg√≥cio e Objetivo

O objetivo deste projeto √© construir um pipeline PySpark modular para processar dados de **Pagamentos** (`.json.gz`) e **Pedidos** (`.csv.gz`) e gerar um relat√≥rio focado em pedidos com falha.

### Crit√©rios de Filtragem (L√≥gica de Neg√≥cios)

O relat√≥rio final (`relatorio_pedidos_2025.parquet`) deve incluir apenas pedidos que satisfa√ßam *todas* as seguintes condi√ß√µes:

1.  **Status do Pagamento:** Pagamento **recusado** (`status` = `false`).
2.  **Avalia√ß√£o de Fraude:** Fraude classificada como **leg√≠tima** (`fraude` = `false`).
3.  **Ano de Refer√™ncia:** Pedidos feitos no ano de **2025**.

O resultado deve ser ordenado por **Estado (UF)**, **Forma de Pagamento** e **Data do Pedido**.

---

## 3. üõ†Ô∏è Guia Passo a Passo: Cria√ß√£o e Desenvolvimento

Esta se√ß√£o detalha a configura√ß√£o e a implementa√ß√£o do projeto, seguindo os padr√µes de **POO** e **Inje√ß√£o de Depend√™ncias**.

### 3.1. Estrutura do Ambiente e Setup

| Passo | Descri√ß√£o | Comandos no Terminal |
| :--- | :--- | :--- |
| **Criar e Estruturar** | Cria o diret√≥rio raiz e todos os pacotes (`src/`, `config/`, `tests/`) necess√°rios para o projeto POO. | `mkdir data-engineering-pyspark`<br>`cd data-engineering-pyspark`<br>`mkdir -p config src/spark_manager src/io src/business_logic src/orchestration tests data/input src/data/output` |
| **Init & Files** | Cria os arquivos `__init__.py` (pacotes) e os arquivos de gerenciamento. | `find . -type d \( -name 'config' -o -name 'src' ... \) -exec touch {}/__init__.py \;`<br>`touch requirements.txt src/main.py` |
| **Setup Python** | Cria o ambiente virtual (`venv`) e instala as depend√™ncias principais (`pyspark`, `pytest`). | `python3 -m venv .venv`<br>`source .venv/bin/activate`<br>`pip install -r requirements.txt` |
| **Datasets** | Confirma que os arquivos `.gz` (2024/2025) est√£o na pasta de entrada. | (Verificar pasta `data/input/`) |

### 3.2. Implementa√ß√£o do C√≥digo (POO)

O projeto √© dividido em classes com responsabilidades √∫nicas, promovendo modularidade:

| Arquivo/Classe | Responsabilidade Principal | Crit√©rios Atendidos |
| :--- | :--- | :--- |
| `config/spark_config.py` | Armazena configura√ß√µes centralizadas (e.g., `target_year=2025`). | **Configura√ß√µes Centralizadas** (4) |
| `src/io/data_io.py` | Realiza I/O. Cont√©m os **Schemas Expl√≠citos** (`ORDERS_SCHEMA`, `PAYMENTS_SCHEMA`). | **Schemas Expl√≠citos** (1), I/**O** (6) |
| `src/business_logic/sales_report_logic.py` | Implementa a filtragem e ordena√ß√£o. Utiliza **Try/Catch** (10) e **Logging** (9) para registro de etapas. | **L√≥gica de Neg√≥cios** (7), Logging (9), Erros (10) |
| `src/orchestration/pipeline_orchestrator.py` | Sequencia a leitura, transforma√ß√£o e escrita do relat√≥rio. | **Orquestra√ß√£o** (8) |
| **Observa√ß√£o:** Todas as classes atendem ao requisito de **Orienta√ß√£o a Objetos** (Crit√©rio 2). |

### 3.3. Inje√ß√£o de Depend√™ncias (DI)

O **`src/main.py`** atua como o **Aggregation Root** (Crit√©rio 3).

1.  **Instancia√ß√£o:** Todas as depend√™ncias (`SparkConfig`, `DataIO`, `SalesReportLogic`, etc.) s√£o criadas em `main.py`.
2.  **Inje√ß√£o:** O `main.py` passa (injeta) essas inst√¢ncias no construtor do `PipelineOrchestrator`, que ent√£o coordena a execu√ß√£o.

---

## 4. üöÄ Execu√ß√£o e Testes Unit√°rios

### 4.1. Execu√ß√£o do Pipeline

A execu√ß√£o √© feita atrav√©s do ponto de entrada do projeto:

```bash
# Executado a partir do diret√≥rio data-engineering-pyspark/
python src/main.py
